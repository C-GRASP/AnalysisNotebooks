{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da00616",
   "metadata": {},
   "source": [
    "## **Compute Phi Percentiles From Measurements (from Sieves etc)**\n",
    "\n",
    "## This notebook serves as a guide of how to calculate values for common, of interest sediment distributions (d10, d50, d90, etc), from datasets that only provide raw measurement data.\n",
    "\n",
    "\n",
    "* To do so, the user (you) enters a dataset where sieved/measured percentiles of grain sizes are provided.\n",
    "    \n",
    "    \n",
    "*  The user will enter in the distributions that you are interested in interpolating from from the dataset \n",
    "\n",
    "    \n",
    "    \n",
    "* For the most accurate interpolation, it is suggested that the user enters in each grainsize provided, as explained in the notebook.\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "## <font color=grey> *This notebooks' output adds new fields to the input sample data dataframe for each specified distribution that the user is interested in interpolating*<font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea9026",
   "metadata": {},
   "source": [
    "## Run these two cells  to get everything set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c26b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8609f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    \"\"\"\n",
    "    response = filename for input\n",
    "    destination = filename for output\n",
    "    \"\"\"    \n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76354f6e",
   "metadata": {},
   "source": [
    "##  **And then Import the Overall Sample Dataset:**\n",
    "### For this example we will be using the publically available sediment data from Woodruff et al., 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d77e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = '1Im9IqxQfEQGdklaSERcurCYXMpU5BJtv'\n",
    "\n",
    "\n",
    "destination = '../data.csv'\n",
    "download_file_from_google_drive(DATASET_ID, destination)\n",
    "df= pd.read_csv(destination)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8047c2f",
   "metadata": {},
   "source": [
    "## In this next cell, we will be using the Numpy interpolation function to calculate the  sediment size of percentiles along the cumulative distribution\n",
    "\n",
    "### You will want to find the column rown numbers within your dataset contain the percentile values for different grainsizes. \n",
    "### <font color=grey> *These columns will typically just be named with a value, denoting grain size. The unit of this grainsize will typically be within the metadata of the downloaded sample file. In some cases, a unit may be provided within the field. In this case, you will need to go into the CSV and rename each column so it is a unitless float value. In the provided example for this notebook, the column names are in millimeters.* <font>\n",
    "    \n",
    "### For the most accurate estimate, you will want to create an bin for each provided measurement within the dataset.\n",
    "    \n",
    "### The output of the bin below will be the estimated, interpolated value for each distribution that you choose (as entered in the prcs variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adacd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=0\n",
    "\n",
    "#This function will loop through each row (sample) in the dataset\n",
    "for i in range(0,len(df)):\n",
    "\n",
    "    #Enter in the percentile values that you wish to calculate within the brackets. \n",
    "    #The ones provided will enable you to calculate any graphical moment \n",
    "    prcs = [.05,.1,.16,.25,.3,.5,.75,.84,.9,.95]\n",
    "    \n",
    "    #Find the position number for columns containing percentiles at grainsize values. Remember that in Python you start counting from zero!\n",
    "    #Some datasets will have more size bins than others. This one has many!\n",
    "    #This variable just extracts the bin size from the column name\n",
    "    grain_size_bins=[float(df.columns[9]), #The first column containing a grain size bin (.06 mm)\n",
    "                     float(df.columns[10]),\n",
    "                    float(df.columns[11]),\n",
    "                     float(df.columns[12]),\n",
    "                     float(df.columns[13]),\n",
    "                     float(df.columns[14]),\n",
    "                     float(df.columns[15]),\n",
    "                     float(df.columns[16]),\n",
    "                     float(df.columns[17]),\n",
    "                     float(df.columns[18]),\n",
    "                     float(df.columns[19]),\n",
    "                     float(df.columns[20]),\n",
    "                     float(df.columns[21]),\n",
    "                     float(df.columns[22]),\n",
    "                     float(df.columns[23]),\n",
    "                     float(df.columns[24]),\n",
    "                     float(df.columns[25]),\n",
    "                     float(df.columns[26]),\n",
    "                     float(df.columns[27]),\n",
    "                     float(df.columns[28]),\n",
    "                     float(df.columns[29]),\n",
    "                     float(df.columns[30]),\n",
    "                     float(df.columns[31]),\n",
    "                     float(df.columns[32]),\n",
    "                     float(df.columns[33]),\n",
    "                     float(df.columns[34]),\n",
    "                     float(df.columns[35]),\n",
    "                     float(df.columns[36]),\n",
    "                     float(df.columns[37]),\n",
    "                     float(df.columns[38]),\n",
    "                     float(df.columns[39]),\n",
    "                     float(df.columns[40]),\n",
    "                     float(df.columns[41]),\n",
    "                     float(df.columns[42]),\n",
    "                     float(df.columns[43]),\n",
    "                     float(df.columns[44]),\n",
    "                     float(df.columns[45]),\n",
    "                     float(df.columns[46]),\n",
    "                     float(df.columns[47]),\n",
    "                     float(df.columns[48]),\n",
    "                     float(df.columns[49]),\n",
    "                     float(df.columns[50]),\n",
    "                     float(df.columns[51]),\n",
    "                     float(df.columns[52]),\n",
    "                     float(df.columns[53]),\n",
    "                     float(df.columns[54]),\n",
    "                     float(df.columns[55]),\n",
    "                     float(df.columns[56]),\n",
    "                     float(df.columns[57]),\n",
    "                     float(df.columns[58]),\n",
    "                    float(df.columns[59])] #The last  column containing a grain size bin (362.04 mm)\n",
    "\n",
    "    #This variable extracts the percentile from each grainsize bin. It moves on to the next sample in each iteration\n",
    "    grain_size_frequencies=[df.loc[i,df.columns[9]],\n",
    "                            df.loc[i,df.columns[10]],\n",
    "                            df.loc[i,df.columns[11]],\n",
    "                            df.loc[i,df.columns[12]],\n",
    "                            df.loc[i,df.columns[13]],\n",
    "                            df.loc[i,df.columns[14]],\n",
    "                            df.loc[i,df.columns[15]],\n",
    "                            df.loc[i,df.columns[16]],\n",
    "                            df.loc[i,df.columns[17]],\n",
    "                            df.loc[i,df.columns[18]],\n",
    "                            df.loc[i,df.columns[19]],\n",
    "                            df.loc[i,df.columns[20]],\n",
    "                            df.loc[i,df.columns[21]],\n",
    "                            df.loc[i,df.columns[22]],\n",
    "                            df.loc[i,df.columns[23]],\n",
    "                            df.loc[i,df.columns[24]],\n",
    "                            df.loc[i,df.columns[25]],\n",
    "                            df.loc[i,df.columns[26]],\n",
    "                            df.loc[i,df.columns[27]],\n",
    "                            df.loc[i,df.columns[28]],\n",
    "                            df.loc[i,df.columns[29]],\n",
    "                            df.loc[i,df.columns[30]],\n",
    "                            df.loc[i,df.columns[31]],\n",
    "                            df.loc[i,df.columns[32]],\n",
    "                            df.loc[i,df.columns[33]],\n",
    "                            df.loc[i,df.columns[34]],\n",
    "                            df.loc[i,df.columns[35]],\n",
    "                            df.loc[i,df.columns[36]],\n",
    "                            df.loc[i,df.columns[37]],\n",
    "                            df.loc[i,df.columns[38]],\n",
    "                            df.loc[i,df.columns[39]],\n",
    "                            df.loc[i,df.columns[30]],\n",
    "                            df.loc[i,df.columns[41]],\n",
    "                            df.loc[i,df.columns[42]],\n",
    "                            df.loc[i,df.columns[43]],\n",
    "                            df.loc[i,df.columns[44]],\n",
    "                            df.loc[i,df.columns[45]],\n",
    "                            df.loc[i,df.columns[46]],\n",
    "                            df.loc[i,df.columns[47]],\n",
    "                            df.loc[i,df.columns[48]],\n",
    "                            df.loc[i,df.columns[49]],\n",
    "                            df.loc[i,df.columns[50]],\n",
    "                            df.loc[i,df.columns[51]],\n",
    "                            df.loc[i,df.columns[52]],\n",
    "                            df.loc[i,df.columns[53]],\n",
    "                            df.loc[i,df.columns[54]],\n",
    "                            df.loc[i,df.columns[55]],\n",
    "                            df.loc[i,df.columns[56]],\n",
    "                            df.loc[i,df.columns[57]],\n",
    "                            df.loc[i,df.columns[58]],\n",
    "                            df.loc[i,df.columns[59]]]\n",
    "    \n",
    "    #Now we interpolate the distribution percentiles \n",
    "    prc_values =  np.interp(prcs,np.hstack((0,np.cumsum(grain_size_frequencies))), np.hstack((0,grain_size_bins)) )\n",
    "    \n",
    "    #Enter in a new row below for each percentile you entered in the prcs brackets. \n",
    "    #Name each variable accordingy in the brackets next to the i (which is the number of row/sample that the loop is interpolating on in the given iteration)\n",
    "   \n",
    "    df.loc[i,[\"d5\"]] = (prc_values[0])\n",
    "    df.loc[i,[\"d10\"]] = (prc_values[1])\n",
    "    df.loc[i,[\"d16\"]] = (prc_values[2])\n",
    "    df.loc[i,[\"d25\"]] = (prc_values[3])\n",
    "    df.loc[i,[\"d30\"]] = (prc_values[4])\n",
    "    df.loc[i,[\"d50\"]] = (prc_values[5])\n",
    "    df.loc[i,[\"d75\"]] = (prc_values[6])\n",
    "    df.loc[i,[\"d84\"]]= (prc_values[7]) \n",
    "    df.loc[i,[\"d90\"]] = (prc_values[8])  \n",
    "    df.loc[i,[\"d95\"]]= (prc_values[9])\n",
    "    \n",
    "    i=i+1\n",
    "\n",
    "\n",
    "\n",
    "write.csv(df, destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
