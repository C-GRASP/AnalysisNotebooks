{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98826cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5e5b5",
   "metadata": {},
   "source": [
    "## This cell navigates to the CUDEM data catalog for the 1/9 arcsecond data and extracts the names of all downloadable files. It then iterates on each name, adding it to a base url and downloading it to the input directory  with wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download CUDEM Files\n",
    "\n",
    "\n",
    "os.chdir('/home/will/Desktop/USGS/CUDEM/Nc')\n",
    "#Extract link to file names from 1/9 arc second resolution server\n",
    "\n",
    "\n",
    "def get_url_paths(url, ext='', params={}):\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.ok:\n",
    "        response_text = response.text\n",
    "    else:\n",
    "        return response.raise_for_status()\n",
    "    soup = BeautifulSoup(response_text, 'html.parser')\n",
    "    parent = [url + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
    "    return parent\n",
    "\n",
    "url = 'https://www.ngdc.noaa.gov/thredds/catalog/tiles/tiled_19as/catalog.html'\n",
    "ext = 'nc'\n",
    "result = get_url_paths(url, ext)\n",
    "\n",
    "#Make links into df\n",
    "link_df = pd.DataFrame(columns = ['link'])\n",
    "link_df['link']=result\n",
    "\n",
    "#split link field by '/' delimiter to get file name\n",
    "\n",
    "link_df['filename']=link_df['link'].str.split('/', expand=True)[9]\n",
    "\n",
    "#download iterating through each file name\n",
    "\n",
    "base_url='https://www.ngdc.noaa.gov/thredds/fileServer/tiles/tiled_19as/' #base url for download\n",
    "i=0\n",
    "for i in range(0,len(link_df)):\n",
    "    file_name=link_df['filename'][i]\n",
    "    dwnld_link=base_url+file_name\n",
    "    !wget {dwnld_link}\n",
    "    i=i+1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e4fda",
   "metadata": {},
   "source": [
    "## This cell converts all of the downloaded cudem nc's into .csv files with fields for latitude, longitude, depth, and crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert downloaded nc files to csv\n",
    "csv_folder='/home/will/Desktop/USGS/CUDEM/'#enter path for csv folder\n",
    "\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith(\".nc\"):\n",
    "        nc = netCDF4.Dataset(os.path.join(os.getcwd(), filename), mode='r')\n",
    "        file_name_no_ext=os. path. splitext(filename)[0]\n",
    "        out_name=csv_folder+file_name_no_ext+'.csv'\n",
    "        df = pd.DataFrame(columns = ['latitude','longitude','depth'])\n",
    "        df['latitude']=nc.variables['lat'][:]\n",
    "        df['longitude']= nc.variables['lon'][:]\n",
    "        df['depth']=nc.variables['Band1'][:]\n",
    "        df['crs']=nc.variables['crs'][:]\n",
    "        df.to_csv(out_name)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d4281",
   "metadata": {},
   "source": [
    "## This cell converts combines all of the  csv's into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12002ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge csv's into one df\n",
    "\n",
    "\n",
    "os.chdir(csv_folder)\n",
    "\n",
    "#create list of files in folder\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "\n",
    "cudem_df=combined_csv \n",
    "\n",
    "#round lat lon to 4th decimal place\n",
    "cudem_df['latitude']=cudem_df['latitude'].round(decimals=3)\n",
    "cudem_df['longitude']=cudem_df['longitude'].round(decimals=3)\n",
    "\n",
    "#create lat lon combined column\n",
    "cudem_df['latlon']= cudem_df['latitude'].map(str)+','+cudem_df['longitude'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120d4d5",
   "metadata": {},
   "source": [
    "## This cell calls your sample data and converts it to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cad727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call in sample dataset\n",
    "\n",
    "# from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    \"\"\"\n",
    "    response = filename for input\n",
    "    destination = filename for output\n",
    "    \"\"\"    \n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "DATASET_ID = '1G9fuC_TjtwTr3JWA85gW7228_Ffxsw0G'\n",
    "\n",
    "\n",
    "destination = csv_folder+'sample_data.csv'\n",
    "download_file_from_google_drive(DATASET_ID, destination)\n",
    "sample_df= pd.read_csv(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1564c5",
   "metadata": {},
   "source": [
    "## This cell merges both output dataframe, effectively adding a depth field to each sample that matches a CUDEM lat/lon point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#round sample lat lon to 4th decimal place\n",
    "sample_df['latitude']=sample_df['latitude'].round(decimals=3)\n",
    "sample_df['longitude']=sample_df['longitude'].round(decimals=3)\n",
    "\n",
    "\n",
    "#create lat lon combined column\n",
    "sample_df['latlon']= sample_df['latitude'].map(str)+','+sample_df['longitude'].map(str)\n",
    "\n",
    "#merge dataframes on latlon column\n",
    "\n",
    "merged_df=pd.merge(sample_df,cudem_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
