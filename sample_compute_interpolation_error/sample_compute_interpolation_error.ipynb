{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3322f11b",
   "metadata": {},
   "source": [
    "# **Compute Interpolation Error for a given Sample Dataset** <font>\n",
    "\n",
    "\n",
    "## This notebook serves to get a sense of interpolation error from Scipy's interpolation function for a dataset of concern/interest\n",
    "\n",
    "* To do so, the user (you)  enters a dataset of concern/interest from the dataset composite (e.g. SandSnap) \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "*  This notebook will then compare the percentile of distributions that were originally provided in the of focus versus the value that scipy's interpolation function would estimate and calculate percent error for each.\n",
    "\n",
    "    \n",
    "    \n",
    "* It is suggested, for an accurate sense of error, to use this notebook on datasets that originally had over 3 distributions. This number can be found in the **num_orig_dists** column, also reported in the notebook.\n",
    "    \n",
    "    \n",
    "\n",
    "## <font color=grey> *This notebooks' output is a dataframe of the user specified input with new columns showing the percent interpolation error for each distribution for each sample.*<font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a7ffe0",
   "metadata": {},
   "source": [
    "## Run these two cells  to get everything set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb816b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    \"\"\"\n",
    "    response = filename for input\n",
    "    destination = filename for output\n",
    "    \"\"\"    \n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132148c",
   "metadata": {},
   "source": [
    "##  **And then Import the Overall Sample Dataset:**\n",
    "### Printed below the code are the datasets to choose from, their respective number of distributions, and which distributions are provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d5601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET_ID = '1G9fuC_TjtwTr3JWA85gW7228_Ffxsw0G'\n",
    "\n",
    "\n",
    "destination = '../data.csv'\n",
    "download_file_from_google_drive(DATASET_ID, destination)\n",
    "df= pd.read_csv(destination)\n",
    "\n",
    "\n",
    "ds=df['dataset'].unique()\n",
    "dd=[]\n",
    "dn=[]\n",
    "\n",
    "i0=0\n",
    "for i0 in range (0, len(ds)):\n",
    "    d=ds[i0]\n",
    "    v=df.loc[df['dataset'] == d, 'num_orig_dists'].unique()[0]\n",
    "    vn=df.loc[df['dataset'] == d, 'Measured_Distributions'].unique()[0]\n",
    "    dd.append(v)\n",
    "    dn.append(vn)\n",
    "    i0=i0+1\n",
    "\n",
    "avail= pd.DataFrame({'dataset': ds, 'num_orig_dists': dd, 'Measured_Distributions':dn}, columns=['dataset', 'num_orig_dists', 'Measured_Distributions'])\n",
    "\n",
    "print(avail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d74f5",
   "metadata": {},
   "source": [
    "## Use this cell to enter your dataset of interest (e.g. sandsnap):\n",
    "#### <font color=red> This is the only cell where you need to enter anything. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest='sandsnap'\n",
    "df=df[df['dataset']==interest]\n",
    "df=df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres=df\n",
    "pres\n",
    "len_df=len(pres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c116f",
   "metadata": {},
   "source": [
    "## This cell will extract the names and values of given percentile distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract number of distributions\n",
    "num_given_dists= int(df['num_orig_dists'][:1])\n",
    "\n",
    "\n",
    "#extract distribution names and distribution percentile\n",
    "given_dist_names=[]\n",
    "given_dist_vals=[]\n",
    "for i in range(0,num_given_dists):\n",
    "    a=(df['Measured_Distributions'][:1]).astype(str).str.split(',', expand=True)[i]\n",
    "    b=a.astype(str).str.split('d', expand=True)[1]\n",
    "    a=a.unique()[0]\n",
    "    val=b.astype(int)/100\n",
    "    val=val.unique()[0]\n",
    "    given_dist_names.append(a)\n",
    "    given_dist_vals.append(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f732a4",
   "metadata": {},
   "source": [
    "## This next cell is where the calculations will occur:\n",
    "* In the outer most for loop, the function is iterating over the number of provided sample distributions. The function is one by one, hiding a distribution from the dataset. This distribution will be re-introduced in the next iteration, and another distribution will be hidden (and so on)\n",
    "\n",
    "* In the loop after that, using the remaining distributions, a value for that temperarily removed, known value is interpolated for each sample.\n",
    "\n",
    "* In the loop nested within the one above, is a function that gathers the percentile value for each distribution for each sample row.\n",
    "\n",
    "## <font color=grey> *The output will be the addition of 2 new columns per input known distribution, the re-calculated value (in '_calc') and the calculated percent error (in '_error')*<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f52eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for n in range (0,num_given_dists): #Repeat for each distribution\n",
    "    \n",
    "    new_dist_names=np.delete(given_dist_names, n)\n",
    "    new_dist_vals=np.delete(given_dist_vals, n)\n",
    "    focus_column=given_dist_names[n]\n",
    "    focus_column_value=given_dist_vals[n]\n",
    "    calc_column=str(given_dist_names[n]+'_calc')\n",
    "    error_column=str(given_dist_names[n]+'_error')\n",
    "    for i in range(0,len_df):#repeat for each row \n",
    "            grain_size_bins=[]\n",
    "            for ia in range(0,(num_given_dists-1)):#repeat for each distribution\n",
    "                bin_size=df[new_dist_names[ia]].iloc[i]\n",
    "                grain_size_bins.append(bin_size)\n",
    "                grain_size_frequencies=new_dist_vals\n",
    "                \n",
    "            #Calculated value    \n",
    "            distribution = scipy.interpolate.interp1d(grain_size_frequencies, grain_size_bins, bounds_error=False, fill_value='extrapolate')\n",
    "            \n",
    "            df.loc[i,[calc_column]] = distribution(given_dist_vals[(n-1)])\n",
    "\n",
    "\n",
    "            \n",
    "    df[error_column]=((df[calc_column]-df[focus_column])/df[focus_column])*100\n",
    "\n",
    "\n",
    "print('Distibutions Interpolation Error Calculated')\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
