{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e6b3f2",
   "metadata": {},
   "source": [
    "# **Compute Interpolation Error for a given Sample Dataset** <font>\n",
    "\n",
    "\n",
    "## This notebook serves to get a sense of interpolation error from Scipy's interpolation function for a dataset of concern/interest\n",
    "\n",
    "* To do so, the user (you)  enters a dataset of concern/interest from the dataset composite (e.g. SandSnap) \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "*  This notebook will then compare the percentile of distributions that were originally provided in the of focus versus the value that scipy's interpolation function would estimate and calculate percent error for each.\n",
    "\n",
    "    \n",
    "    \n",
    "* It is suggested, for an accurate sense of error, to use this notebook on datasets that originally had over 3 distributions. This number can be found in the **num_orig_dists** column reported below.\n",
    "    \n",
    "    \n",
    "\n",
    "## <font color=grey> *This notebooks' output is a dataframe of the user specified input with new columns showing the percent interpolation error for each distribution for each sample.*<font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa587c98",
   "metadata": {},
   "source": [
    "## Run these two cells  to get everything set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6997b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5be2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    \"\"\"\n",
    "    response = filename for input\n",
    "    destination = filename for output\n",
    "    \"\"\"    \n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a81408",
   "metadata": {},
   "source": [
    "##  **And then Import the Overall Sample Dataset:**\n",
    "### Printed below the code are the datasets to choose from, their respective number of distributions, and which distributions are provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c290f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET_ID = '1G9fuC_TjtwTr3JWA85gW7228_Ffxsw0G'\n",
    "\n",
    "\n",
    "destination = '../data.csv'\n",
    "download_file_from_google_drive(DATASET_ID, destination)\n",
    "df= pd.read_csv(destination)\n",
    "\n",
    "\n",
    "ds=df['dataset'].unique()\n",
    "dd=[]\n",
    "dn=[]\n",
    "\n",
    "#This part aggregates the unique dataset names, the amount of sample percentile distributions provided in that dataset, and which ones\n",
    "i0=0\n",
    "for i0 in range (0, len(ds)):\n",
    "    d=ds[i0]\n",
    "    v=df.loc[df['dataset'] == d, 'num_orig_dists'].unique()[0]\n",
    "    vn=df.loc[df['dataset'] == d, 'Measured_Distributions'].unique()[0]\n",
    "    dd.append(v)\n",
    "    dn.append(vn)\n",
    "    i0=i0+1\n",
    "\n",
    "avail= pd.DataFrame({'dataset': ds, 'num_orig_dists': dd, 'Measured_Distributions':dn}, columns=['dataset', 'num_orig_dists', 'Measured_Distributions'])\n",
    "\n",
    "print(avail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e71372",
   "metadata": {},
   "source": [
    "## Use this cell to enter your dataset of interest (e.g. sandsnap):\n",
    "#### <font color=red> This is the only cell where you need to enter anything. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb44beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is where you type in the dataset of interest (found in the table above)\n",
    "interest='sandsnap'\n",
    "\n",
    "#This subsets the larger dataset composite just to just your dataset of interest\n",
    "df=df[df['dataset']==interest]\n",
    "df=df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c5491",
   "metadata": {},
   "source": [
    "## This cell will extract the names and values of given percentile distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e513c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this counts the number of distributions for your dataset of interes\n",
    "num_given_dists= int(df['num_orig_dists'][:1])\n",
    "\n",
    "\n",
    "#extract distribution names and distribution percentiles that were provided with the source dataset (e.g, 'd50' and .5)\n",
    "given_dist_names=[]\n",
    "given_dist_vals=[]\n",
    "for i in range(0,num_given_dists):\n",
    "    a=(df['Measured_Distributions'][:1]).astype(str).str.split(',', expand=True)[i]\n",
    "    b=a.astype(str).str.split('d', expand=True)[1]\n",
    "    a=a.unique()[0]\n",
    "    val=b.astype(int)/100\n",
    "    val=val.unique()[0]\n",
    "    given_dist_names.append(a)\n",
    "    given_dist_vals.append(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bee8ab",
   "metadata": {},
   "source": [
    "## This next cell is where the calculations will occur:\n",
    "* In the outer most for loop, the function is iterating over the number of provided sample distributions. The function is one by one, hiding a distribution from the dataset. This distribution will be re-introduced in the next iteration, and another distribution will be hidden (and so on)\n",
    "\n",
    "* In the loop after that, using the remaining distributions, a value for that temperarily removed, known value is interpolated for each sample.\n",
    "\n",
    "* In the loop nested within the one above, is a function that gathers the percentile value for each distribution for each sample row.\n",
    "\n",
    "## <font color=grey> *The output will be the addition of 2 new columns per input known distribution, the re-calculated value (in '_calc') and the calculated percent error (in '_error')*<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f52eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for n in range (0,num_given_dists): #Repeats for each distribution\n",
    "    # \"deleting\" the distribution name and value to be recalculated\n",
    "    new_dist_names=np.delete(given_dist_names, n)\n",
    "    new_dist_vals=np.delete(given_dist_vals, n) \n",
    "    # \"preserving\" the distribution name to be recalculated as another variable\n",
    "    focus_column=given_dist_names[n] \n",
    "    focus_column_value=given_dist_vals[n] \n",
    "    #new columns for recalculated value and error\n",
    "    calc_column=str(given_dist_names[n]+'_calc')  \n",
    "    error_column=str(given_dist_names[n]+'_error')\n",
    "    for i in range(0,len_df):#repeats for each row, aka sample \n",
    "            grain_size_bins=[]\n",
    "            #This collects the values from the left over \"original\" distributions\n",
    "            for ia in range(0,(num_given_dists-1)):\n",
    "                bin_size=df[new_dist_names[ia]].iloc[i] \n",
    "                grain_size_bins.append(bin_size)\n",
    "                grain_size_frequencies=new_dist_vals\n",
    "                \n",
    "            #This interpolates the value using the gathered \"original\" distributions from above\n",
    "            distribution = scipy.interpolate.interp1d(grain_size_frequencies, grain_size_bins, bounds_error=False, fill_value='extrapolate')\n",
    "            \n",
    "            #This adds them to the new calcualted column\n",
    "            df.loc[i,[calc_column]] = distribution(given_dist_vals[(n-1)])\n",
    "\n",
    "\n",
    "    #This calculates the percent error:        \n",
    "    df[error_column]=((df[calc_column]-df[focus_column])/df[focus_column])*100\n",
    "\n",
    "\n",
    "print('Distibutions Interpolation Error Calculated')\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
