{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<tr style=\"vertical-align: top; padding: 0; margin: 0;background-color: #ffffff\">\n",
    "        <td style=\"vertical-align: top; padding: 0; margin: 0; padding-right: 15px;\">\n",
    "    <p style=\"background: #182AEB; color:#ffffff; text-align:justify; padding: 10px 25px;\">\n",
    "        <strong style=\"font-size: 1.0em;\"><span style=\"font-size: 1.2em;\"><span style=\"color: #ffffff;\">The Coastal Grain Size Portal (C-GRASP) dataset <br/><em>Will Speiser, Daniel Buscombe, Evan Goldstein</em></strong><br/><br/>\n",
    "        <strong>> Interpolate Percentiles from Other Dataset Percentiles </strong><br/>\n",
    "    </p>                       \n",
    "        \n",
    "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #ff5733\">The purpose of this notebook</strong>  \n",
    "    <br/><font color=grey> TThis notebook will output a dataframe containing all of the data from a chosen C-GRASP dataset with  new fields containing an estimated percent error for interpolation of distribution percentiles. This will only be calculated for samples where distribution percentile values are included in the source dataset, as that is the only way to establish a \"known\" value. As C-Grasp file sizes vary completion of this task will vary with internet connectivity and computer processing power.<font><br/>\n",
    "    <br/><font color=grey> This notebook provides simple code that estimates the percent error for various interpolated distribution values in the C-Grasp dataset.<font><br/>    \n",
    "    <br/><font color=grey> To do so, a user choose a CGRASP dataset of choice . <font><br/>\n",
    "    <br/><font color=grey> The notebook then runs loops through each sample with known distribution percentile values, recalculates that value and calculates an estimate for percent error of the scipy interpolation function (see the \"sample_compute_percentile\" notebook).<font><br/>    \n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "import requests\n",
    "import ipywidgets\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset collection widget\n",
    "zen=ipywidgets.Select(\n",
    "    options=['Entire Dataset', 'Estimated Onshore Data', 'Verified Onshore Data', 'Verified Onshore Post 2012 Data'],\n",
    "    value='Entire Dataset',\n",
    "    # rows=10,\n",
    "    description='Dataset:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(zen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://zenodo.org/record/5874231/files/' \n",
    "if zen.value=='Entire Dataset':\n",
    "    filename='dataset_10kmcoast.csv'\n",
    "if zen.value=='Estimated Onshore Data':\n",
    "    filename='Data_EstimatedOnshore.csv'\n",
    "if zen.value=='Verified Onshore Data':\n",
    "    filename='Data_VerifiedOnshore.csv'\n",
    "if zen.value=='Verified Onshore Post 2012 Data':\n",
    "    filename='Data_Post2012_VerifiedOnshore.csv'\n",
    "print(\"Downloading {}\".format(url+filename))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will download the CGRASP dataset and read it in as a pandas dataframe with variable name `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=(url+filename)\n",
    "print('Retrieving Data, Please Wait')\n",
    "#retrieve data\n",
    "df=pd.read_csv(url)\n",
    "print('Sediment Data Retrieved!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at what distributions are provided from source data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_values=np.array2string(df['Measured_Distributions'].unique()) #Find each distribution in entire dataset that was provided provided in source data for at least one sample\n",
    "given_values= given_values[:].replace(\" \",\",\").replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\") #convert to string and remove array artefacts\n",
    "given_values=(list(set(given_values.split(',')))) #extract delete duplicates (i.e. when multiple source datasets provide the same  distribution)\n",
    "given_values.remove('nan') #remove nan from list\n",
    "given_values=np.array(given_values) #Turn it into an array for use later\n",
    "print('Given distribution values from source data in dataset: ', given_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new, blank calculated interpolation value and percent error columns for each of those distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range (0,len(given_values)):\n",
    "            calc_column=str(given_values[d]+'_calc')  \n",
    "            error_column=str(given_values[d]+'_error')\n",
    "            df[calc_column]='' \n",
    "            df[error_column]='' \n",
    "            d=d+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This next cell is where the calculations will occur:\n",
    "* In the outer most for loop, the function is iterating over each sample and accounting for the number of distributions provided in its source data.\n",
    "\n",
    "\n",
    "* For the next loop within the previous one, the value and name of each distribution provided in the source data is being collected\n",
    "\n",
    "* In the next loop,the function is one by one \"hiding\" a distribution from the dataset and is re-interpolated from the other distributions from the source data. This distribution is re-introduced in the next iteration, and another distribution is hidden/re-interpolated. These re-interpolated values go in the \"_calc\" columns.\n",
    "\n",
    "* After that, the percent error of each re-interpolated distribution value is calculated with the distribution value from the source data\n",
    "\n",
    "## <font color=grey> *The output will be the addition of 2 new columns distribution provided in a sample's source data, the re-interpolated value (in '_calc') and the calculated percent error (in '_error')*<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in range (0,len(df)): #loop on each sample\n",
    "    if df['num_orig_dists'].iloc[z] < 3: #if the number of given distributions is less than 3 skip the sample\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            num_orig_dists=df['num_orig_dists'].iloc[z]#extract amount of known distributions per sample\n",
    "            given_dist_names=[]\n",
    "            given_dist_vals=[]\n",
    "            i=0\n",
    "            for i in range(0,num_orig_dists): #find distribution values provided in source data for each sample\n",
    "                a=(df['Measured_Distributions'].iloc[z]) #extract sample's provided distributions\n",
    "                a=a.split(',')[i] #extract distribution focused on in this iteration\n",
    "                b=a.split('d')[1] #pull number value from name\n",
    "                val=int(b)/100 #turn value to decimal (e.g. 90 to .9)\n",
    "                given_dist_names.append(a) #collect given distribution names from each sample\n",
    "                given_dist_vals.append(val) #collect given distribution values from each sample\n",
    "            i=0   \n",
    "     \n",
    "            for n in range (0,num_orig_dists): #Repeats for each distribution\n",
    "                    # \"deleting\" the distribution name and value to be recalculated\n",
    "                    new_dist_names=np.delete(given_dist_names, n)\n",
    "                    new_dist_vals=np.delete(given_dist_vals, n) \n",
    "                    # \"preserving\" the distribution name to be recalculated as another variable\n",
    "                    focus_column=given_dist_names[n] \n",
    "                    focus_column_value=given_dist_vals[n]\n",
    "                    calc_column=str(given_dist_names[n]+'_calc')  \n",
    "                    error_column=str(given_dist_names[n]+'_error')\n",
    "                    #new columns for recalculated value and error\n",
    "\n",
    "\n",
    "\n",
    "                    grain_size_bins=[]\n",
    "                    ia=0\n",
    "                    for ia in range(0,(num_orig_dists)):\n",
    "                        bin_size=df[new_dist_names[ia]].iloc[z] \n",
    "                        grain_size_bins.append(bin_size)\n",
    "\n",
    "                    grain_size_frequencies=new_dist_vals\n",
    "                     #This interpolates the value using the gathered \"original\" distributions from above\n",
    "                    distribution = scipy.interpolate.interp1d(grain_size_frequencies, grain_size_bins, bounds_error=False, fill_value='extrapolate')\n",
    "                    #This adds them to the new calculated column\n",
    "                    df.loc[z,[calc_column]] = float(distribution(given_dist_vals[(n)]))\n",
    "                    df.loc[z, error_column]=abs(((df[calc_column].iloc[z]-df[focus_column].iloc[z])/df[focus_column].iloc[z])*100)\n",
    "        except:\n",
    "            pass\n",
    "print('Error Calculation Successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if that worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start=len(df.columns)-(len(given_values)+9)\n",
    "df.iloc[:, start:len(df.columns)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define a csv file name for the output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csvfile='../data_interp_error.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the data to that csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_csvfile) #convert data to CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
